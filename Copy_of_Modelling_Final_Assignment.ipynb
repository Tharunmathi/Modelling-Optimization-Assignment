{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCKrgxyRSS8p",
        "outputId": "b34b6d8c-72a6-47d2-9470-1c0219041701"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/gaussian_process/_gpr.py:660: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
            "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  _check_optimize_result(\"lbfgs\", opt_res)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 10.0. Increasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/gaussian_process/_gpr.py:660: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
            "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  _check_optimize_result(\"lbfgs\", opt_res)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/gaussian_process/kernels.py:452: ConvergenceWarning: The optimal value found for dimension 0 of parameter k1__constant_value is close to the specified upper bound 10.0. Increasing the bound and calling fit again may find a better value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/gaussian_process/_gpr.py:660: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
            "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "  _check_optimize_result(\"lbfgs\", opt_res)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, Matern\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the white wine dataset\n",
        "white_wine_data = pd.read_csv('/content/winequality-white.csv', sep=';')\n",
        "\n",
        "# Check for missing values and handle them (if any)\n",
        "if white_wine_data.isnull().sum().any():\n",
        "    white_wine_data = white_wine_data.fillna(white_wine_data.mean())\n",
        "\n",
        "# Separate features (X) and target (y)\n",
        "X_white = white_wine_data.drop('quality', axis=1)\n",
        "y_white = white_wine_data['quality']\n",
        "\n",
        "# Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_white, y_white, test_size=0.2, random_state=42)\n",
        "\n",
        "# -------- Gaussian Process Regression Model --------\n",
        "# Define the kernel: product of constant kernel and RBF kernel\n",
        "kernel = C(1.0, (1e-4, 1e1)) * RBF(1.0, (1e-4, 1e1))\n",
        "\n",
        "# Instantiate the GaussianProcessRegressor with the chosen kernel\n",
        "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
        "\n",
        "# Fit the model to the training data\n",
        "gp.fit(X_train, y_train)\n",
        "\n",
        "# Cross-validation to evaluate model performance more robustly\n",
        "cross_val_score_result = cross_val_score(gp, X_white, y_white, cv=3, scoring='neg_mean_squared_error')\n",
        "mean_cross_val_score = np.mean(cross_val_score_result)\n",
        "print(f\"Mean Cross-Validation MSE (GPR): {-mean_cross_val_score}\")\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred_gpr = gp.predict(X_test)\n",
        "\n",
        "# Evaluate the GPR model using Mean Squared Error (MSE)\n",
        "mse_gpr = mean_squared_error(y_test, y_pred_gpr)\n",
        "print(f\"Mean Squared Error on Test Set (GPR): {mse_gpr}\")\n",
        "\n",
        "# -------- Naive Bayes Model (Bayesian Approach) --------\n",
        "# Binarize wine quality for classification (high vs low quality)\n",
        "y_binarized = (y_white >= 6).astype(int)  # Classifying high vs low quality (0 = low, 1 = high)\n",
        "\n",
        "# Normalize the data before applying Naive Bayes\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_white)\n",
        "\n",
        "# Train Naive Bayes model (simplified Bayesian approach)\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_scaled, y_binarized)\n",
        "\n",
        "# Make predictions with the Naive Bayes model\n",
        "y_pred_nb = nb.predict(X_scaled)\n",
        "\n",
        "# Evaluate the Naive Bayes model using accuracy\n",
        "accuracy_nb = accuracy_score(y_binarized, y_pred_nb)\n",
        "print(f\"Accuracy of Naive Bayes Model: {accuracy_nb}\")\n",
        "# 1. Histogram of wine quality distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y_white, bins=10, color='skyblue', edgecolor='black')\n",
        "plt.xlabel('Wine Quality')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Wine Quality')\n",
        "plt.show()\n",
        "\n",
        "# 2. Residual Plot for GPR\n",
        "residuals_gpr = y_test - y_pred_gpr\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_pred_gpr, residuals_gpr, alpha=0.6)\n",
        "plt.axhline(y=0, color='red', linestyle='--')\n",
        "plt.xlabel('Predicted Quality (GPR)')\n",
        "plt.ylabel('Residuals (True - Predicted)')\n",
        "plt.title('Residuals: Predicted vs True Values (GPR)')\n",
        "plt.show()\n",
        "\n",
        "# 3. True vs Predicted Wine Quality (GPR)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(y_test, y_pred_gpr, alpha=0.6, label=\"GPR Predictions\")\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--')  # Line of perfect prediction\n",
        "plt.xlabel('True Quality')\n",
        "plt.ylabel('Predicted Quality (GPR)')\n",
        "plt.title('True vs Predicted Wine Quality (GPR)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 4. Feature Importance Visualization (for Naive Bayes)\n",
        "feature_importance = np.abs(nb.theta_).mean(axis=0)  # Taking the mean of feature coefficients\n",
        "features = X_white.columns\n",
        "sorted_idx = np.argsort(feature_importance)[::-1]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.barh(features[sorted_idx], feature_importance[sorted_idx], color='teal')\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Feature Importance (Naive Bayes)')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}